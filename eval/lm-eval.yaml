model: vllm
model_args:
  pretrained: state-spaces/mamba-2.8b-hf
  tokenizer: state-spaces/mamba-2.8b-hf
  tensor_parallel_size: 4
  trust_remote_code: true
  max_model_len: 32768
  dtype: bfloat16
tasks:
  - gsm8k
  - mmlu_pro
output_path: ./results/lm-eval/mamba-2.8b
wandb_args:
  project: eval-mamba
  entity: mamba-monks
  dir: ./outputs/lm-eval
